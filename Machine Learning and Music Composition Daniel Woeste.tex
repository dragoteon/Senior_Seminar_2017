
\documentclass{sig-alternate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}

%%does there happen to be this weird thing called spellcheck for LaTeX?

\begin{document}
% --- Author Metadata here ---

\conferenceinfo{UMM CSci Senior Seminar Conference, November 2017}{Morris, MN}
\title{Machine Learning and Music Composition}
\numberofauthors{1}
\author{
\alignauthor
Daniel Woeste\\
	\affaddr{Division of Math and Sicence}\\
	\affaddr{University of Minnesota, Morris}\\
	\affaddr{Morris, Minnesota, USA 56267}\\
	\email{woest015@morris.umn.edu}
}


\maketitle
\begin{abstract}
Throughout history, musical composition has been thought of as limited to those as gifted with some higher insight. As computers have risen in prominence, they have also asserted themselves as an almost necessary tool used in the field of music. Programs such as Finale and Sibelius are used by a composer for music writing and recording. Recent advancements in machine learning technologies has possibly provided a new way for computers to be used in the field of music, as well as possibly bringing music composition to the masses. Machine learning offers the unique possibility of having a program generate the song, leaving the user to edit or pick the song closest to their musical tastes. 

\end{abstract}

\keywords
Random Forests, Markov Chains, Neural Networks, Machine Learning

\section{Introduction}
\label{sec:introduction}

\section{Background}
\label{sec:background}
	We use this section to introduce important terminology for the rest of the paper.

\subsection{Music}
\label{sec:music}
	In this section we explain the terminology that is important to the music side of the paper.
\subsubsection{Melody}
\label{sec:melody}
A melodic progression is the intervall traveled when changing from one note to another. These intervals are broken down into whole steps and half steps. Combining several progressions together produces a melody. A melody is a combination of pitches, rhythms and note duration, considered pleasing to the ear of the listener. At any point during a piece of music, the most important part played is considered the melody. 

Chords are when several notes are being played at the same time. This can have a pleasing effect known as harmony, otherwise known as consonace, or a clashing effect known as dissonance. While most songs tend to be harmonic, dissonance gives the music drive and forward motion. Without dissonance, music lacks a sense of progression. Music is given a sense of forward motion by the resolution from dissonant chords to harmonic chords. Dissonance, in a simple sense, gives the music a direct path to follow, moving from a clash to peace and harmony

Musical texture also plays an important role in the style of music generated by machine learning programs. Homophonic and polyphonic textures are two types of music textures, which will be discussed in greater detail later. A homophonic texture is a piece of music containing one melody played at a time. The homophonic melody can be played by more than one instrument simultaneously, but there is no competing melody being played at the same moment. Polyphonic means there is more than one melodic lines being played simultaneously. Polyphonic music is a more common style to jazz and contemporary pieces. Simpler classical music and pop music tend to favor homophonic texture due to its easier to understand nature.

\subsection{General Framework}
\label{sec:framework}
%%insert figure relevant to this section
%%Move the general framework section into the 3rd section.
	All of the methods described in this paper share a similar framework that would be useful to understand. The commonality between all of the methods is that every note is generated in relation to a sequence of previous notes. Sometimes the new note is only dependant on a single previous note, while in other circumstances it depends on a larger set of notes. This method of relying on previous notes allows the program to generate new notes that fit well with the current melodic progression. Similar to how a sentence is formed, each new word is dependent on the previous word to make a coherent thought. A coherent melody is created by using notes that blend well with the overall theme already being expressed. 

\subsection{Machine Learning}
\label{sec:machinelearning}
	Machine learning, a subcategory of artificial intelligence, is an area of computer science that focuses on programs that are able to systematically change their behavior. These learning systems are able to make alterations, or mutations, during a time of training. Alterations to the system are made by using existing musical melodies. These melodies are used as a ``final answer'' that the program can use to score itself against.  Using multiple passes over the training melodies, the program slowly approaches a correct answer by comparing new changes to the answer. After an unknown period of time, the program is able to generate melodies that are similar in style, but not identical to the training data set. Using a machine learning method frees the system from possible biases of the programmer. This style of data driven learning allows machine learning programs to be well suited for applications such as self-driving cars, text prediction, online recommendations, and even the field of music composition.
	
	In the remainder of this section we will describe 

\subsubsection{Decision Trees}
\label{sec:decision_trees}

\begin{figure}[p]
	\includegraphics[width=\linewidth]{"./Graphics/Descision_tree_example".jpg}
	\caption{Decision tree example}
	\label{fig:decisiontree}
\end{figure}

Decision trees are structures that represent choices and their possible outcomes in a branching style, see~\ref{fig:decisiontree}. Every node on a decision tree represents a choice, such as a note length to follow the previous note. In Figure~\ref{fig:decisiontree} the nodes are represented by the blue circles representing a current note. The branches out from the nodes represent the possible choices that could be made for note lengths. The decision tree branches out to include all possible outcomes for every choice on the tree.  Starting from the top of the tree would place us at the start of a rhythmic pattern. Progressing through the tree also progresses the rhythmic pattern note by note. 


%	Decision trees are a structure that represent the tests and outcome of an algorithm. Every node on the decision tree represents a possible test or choice that an algorithm can make. The resulting branches coming from that node represent all of the possible outcomes of that test. This can easily be represented by flipping a coin multiple times in a row. Every time the coin is flipped, a node is created in the decision tree. In this instance, the two nodes for the coin flip are heads and tails. For every subsequent coin flip, a new set of nodes and leaves are generated. As a result the decision tree contains all possible outcomes for the Nth coin flip.

\subsubsection{Overfitting}
\label{sec:overfitting}
	A possible side effect with decision trees and all types of machine learning is overfitting. Machine learning algorithms and programs, such as decision trees, develop overfitting because the algorithm itself becomes too complex. Through multiple iterations of training, the algorithm develops too many parameters that are overly specific to the training data set. Meaning that the program that has developed is not able to accurately work on data that it was not trained on.  Overfitting results in a loss of accuracy due to program being so complex that it cannot accurately predict new results. 


%%When a decision tree is made and for an application, it can become too complex and dependent on that particular training data set. This occurs when a tree gains too many parameters, making the tree too specific to the data that it was trained on, or the nodes the tree branches from are inconsequential to the actual results. One consequence of overfitting occurs when the tree is subseqently tried on data different from what it was trained on. Thus, tree loses its accuracy at predicting the possible results.


\subsubsection{Finite Automata}
\label{sec:finiteautomata}
	Finite automata, otherwise known as finite state machines, are representations of a an abstract computational model that can exist in only one state at a time. The transition from one state to another is caused by external inputs to finite automata. Every finite automata has a start state, denoted by the arrow head, and an end state, denoted by the double circle. The finite automata listen below deviates from these notations, the start state is the state with the value of negative four, and the end state is the state with the value of four. The values one, two, and four represent quarter notes, half notes, and whole ntoes respectively.

\section{Methods}
\label{sec:methods}
	In this section, we will be describing three separate subcategories of machine learning. The three subcategories of machine learning described are \textit{random forests},\textit{ markov chains}, and \textit{neural networks}.

%%Need to chance will be describing to we describe.

	First, we discuss the use of random forests. Random forests operate by generating and altering a large collection of decision trees. The generated output of the random forest results from the average answer of all decision trees in the forest. As with all machine learning a set of amount of time is required to teach the algorithm to properly generate answers. For the case of random forest, each decision tree is made on a small part of the data set, with overlap among all of the trees. This overlap in the data that the trees are built on helps to reduce over fitting of the trees to the data. When it comes to music, a program known as ALYSIA, uses two random forests to generate music from a given set of lyrics.

	In comparison to random forests, markov chains function differently. Markov chains use a state machine and a statistical models to predict the next value. As demonstrated in Figure 1, a simple finite automata generates the note length, but not pitch values for a song. The values 1, 2, and 4 represent quarter, half, and whole notes respectively. The value of -4 is a starting state that holds no value musically, other than a place to start the song. The automata visualizes the way the markov chain uses probabilistic determination to predict what the next note or value will be. In the terms of music, this means for each state in the automata, it has a percentage chance of moving to any of the other nodes in the automata. In this case, the first note will be a quarter note with an 80\% chance of having a repeated quarter note, with only a 10\% chance of a subsequent half note.
\todo[inline]{introduce the word trained instead of generated to make sense with the neural networks paragraph}	


\begin{figure}[H]
	\includegraphics[width=\linewidth]{"./Graphics/neural_network_diagram".jpg}
	\caption{example of a neural network}
	\label{fig:neuralnetexample}
\end{figure}
	

\begin{figure}[H]
	\includegraphics[width=\linewidth]{"./Graphics/Markov Chain Jflap".jpg}
	\caption{Finite automata detailing the Markov Generated in Automatic Composition of Music with Methods of Computational Intelligence}
	\label{fig:markovchain1}
\end{figure}

	Another approach to computer generation of music is neural networks, which is a deterministic style of algorithm. They are a style of machine learning that is based upon the functions of the human brain. This is accomplished by generating many highly interconnected nodes (neurons) that are working together to solve a single problem. Similar to random forests, neural networks are trained upon a given set of data to produce a network that will produce a favorable. Unlike the previous two methods, neural networks are almost always deterministi. This means for the same input given, a neural network will always have the same output. This differs fundamentally from the other methods which rely heavily on elements of chance to determine an outcome.

\section{Methods and Music}
	In this section, we will discuss how the previously stated methods can be applied to the composition of music with specific examples.
\subsection{ALYSIA}
\label{sec:ALYSIA}
	ALYSIA, Automated LYrical Song-writing Application, is a program that uses random forests to generate songs and melodies for a given set of pop lyrics. ALYSIA contains two different random forests that work in tandem: one to produce rhythm, the other to produce pitch values. [ALYSIA] Random forests can provide some inherent benefits over Markov chains. Even though both models work from a cache of previous notes, and both predict what the next note should be, random forests and markov chains function based on different models. Markov chains offer only a statistical probability of what the next note should be. While random forests include a reasoning to what the next note should be.
	
	ALYSIA is able to encode a reason for the next note; instead of relying on entirely random chance for the next note, it uses a system known as function known as feature extraction to determine the shape of the background music that the ALYSIA is trying to compose music to. The feature extractions includes parameters such as key signature, time signature, note duration, and many others. It additionally includes feature extraction for the lyrics that ALYSIA is trying to match. The feature extraction includes parameters such as, word frequency, and even word vowel strength (value extracted from CMUDict v0.07 database.) This function allows ALYSIA to examine the previous few notes of the melody, if not the entire melody, in order to predict what the best next note will be.
\todo[inline]{talk about the co creative nature of ALYSIA mentioned in the paper}

\subsection{LSTM and RNN}
\label{sec:lstm}
 	Some neural networks that were used to generate an entirely different style of music: polyphonic music. Polyphonic music is a more complicated style of music, containing multiple melody lines played at once. This adds an additional level of complication to what the neural network has to keep track of due to complexities of music in relation to music patterns and harmonic relations between the multiple lines of music. The paper by Daniel D. Johnson covers several types of neural networks including \textit{recurrent neural networks} (RNN) based upon the \textit{long short-term memory} system (LSTM).

	 A recurrent neural network is similar to the neural networks described above. The difference between the two is that instead of having several layers of nodes that always progress from one layer to next, in a forward motion, recurrent networks allow for backwards travel along the network. This means that as a node completes an operation, the transition, as indicated by the arrow pointing to the next node, might not point at a node in the next layer. It can transition back to the node itself or to a node in the previous layers.
	
	Long short-term memory is a specific alteration to RNN style that allows the program to store dependencies, what can be considered as previous context, throughout iterations of the network. This type of model is particularly well suited for the generation of much because every note is dependent upon previous notes to make a current melody. The long term memory of the network determines  context of the music, better allowing the program to produce a coherent subsequent note. For example, an RNN with LSTM could remember what key the music is in, for instance the music is in C major, and it would additionally be able to remember key musical features such an ascending melodic line. With this knowledge the network could determine that the appropriate following action might be a descending musical line.

\subsection{Markov Chains}
\label{sec:markovmethods}
	The program based upon markov chains in Klinger et al produces melodic lines by running the produced music through multiple iterations of the program. Each iteration of the music focuses on the generation of a new layer of music. The first iteration of the music is used to generate a simple rhythmic pattern for the rest of the program to alter. Figure~\ref{fig:markovchain1} shows a simplistic version of a markov chain generated to produce simplistic patterns of only quarter and half notes, with a melody that always ends in a whole note.

	The resulting output from the rhythmic markov chain is then piped into a new markov chain that sets the pitches from the rhythm. To accomplish this, the program could use two different styles of markov chains. They are stated as an absolute or a relative markov chain in the Klinger paper. The difference between the two styles of markov chains is that for the absolute chain, it functions purely by choosing a note value to work with. In other words, it chooses one of the available notes in the octave (CDEFGABC) to use as the next note value. Conversely the relative markov chain uses interval length, or the distance from one pitch to the next to determine the next note. So instead of generating an A to a B transition, the relative markov chain would output a one-step transition. The benefit of the relative style is that it is less work to transpose the music from one to another; everything is based upon step distance rather than absolute note values.

\begin{figure}[H]
\centering
	\includegraphics[width=(\linewidth/2)] {"./Graphics/Markov Chain output 2".jpg}
	\caption{A single bar melody produced after the second iteration of markov chains}
	\label{fig:markovmelody2}
\end{figure}

	The last step to the markov model is labeled as post processing. The post processing of the music is a step that takes the already generated simplistic melody and then runs it through a last markov chain in an attempt the complicate the melody. The complication of the melody is an attempt to make the music more interesting to the listener. The post processing can be run as many times as the user feels necessary. The post processing can achieve these alteration through several methods: note splitting, one point mutation, and recombination.

\begin{figure}[H]
\centering
	\includegraphics[width=(\linewidth/2)] {"./Graphics/Markov Chain output 3".jpg}
	\caption{Previous melody after being put through one-point mutation}
	\label{fig:markovmelody3}
\end{figure}

	One-point mutation is process of taking the melody and traversing over it with a low chance for each note to change pitch value.
	
	Note splitting is the similar to the one-point notation in the fact that it is traversal over the melody, but instead of the changing the pitch value of the note, it splits the note into an equal value of shorter duration notes. For example note splitting could change a quarter note into a run sixteenth notes.

\begin{figure}[H]
\centering
	\includegraphics[width=(\linewidth/2)] {"./Graphics/Markov Chain output 4".jpg}
	\caption{Previous melody after being put through a note-splitting mutation}
	\label{fig:markovmelody4}
\end{figure}

	Recombination works in an entirely different way than the previous two methods. The concept of recombination is taking two original parent melodies and combining them into a single child melody.

	
	

\label{sec:methodsandmusic}
\subsection{Citations}
\label{sec:citations}

\section*{Acknowledgments}
\label{sec:acknowledgments}


\todo[inline]{Fix the section heads because currently they are not organized in a way that makes sense.}
\todo[inline]{the picture on descision trees needs to be smaller than it is currently}
\todo[inline]{I know the markov chain graphic is in a very weird spot, but there was a weird spacing issue with it earlier so currently i have it placed there to get rid of the spacing problem}






\bibliographystyle{abbrv}
%% Need to do proper citations and bibliography But I ran out of time. Will do for next iteration. 
\bibliography{sample_paper}  


\end{document}
